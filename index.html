
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>VolDART</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://voldart-dh.github.io/img/overview_combined.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1711">
    <meta property="og:image:height" content="576">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://voldart-dh.github.io/"/>
    <meta property="og:title" content="Synthesizing Interactive Human Behaviors" />
    <meta property="og:description" content="3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches."/>

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ReconFusion: 3D Reconstruction with Diffusion Priors" />
    <meta name="twitter:description" content="3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches."/>
    <meta name="twitter:image" content="https://reconfusion.github.io/img/overview_combined.png" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤”</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
	<link rel="stylesheet" href="css/fontawesome.all.min.css">
	<link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">


	<!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8ZERS5BVPS"></script>
  <script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-8ZERS5BVPS');
  </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
	<script defer src="js/fontawesome.all.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.5.0/Chart.min.js"></script>

    <script src="js/app.js"></script>
    <script src="js/synced_video_selector.js"></script>

</head>

<body style="padding: 1%; width: 100%">
    <div class="container-lg text-center" style="max-width: 1500px; margin: auto;" id="main">
    <!-- <div class="container" id="main"> -->
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Synthesizing Interactive Human Behaviors</b></br>
            </h2>
        </div>
        <div class="row text-center">
<div class="col-md-3">
    </div>
            <div class="col-md-6 text-center">
                <ul class="list-inline">
                    <li>
                            Jixian Tang
                        <sup>1, 2</sup>
                    </li>
                    <li>
                            Xiangyi Jia
                       <sup>1</sup>
                    </li>
                    <li>
                            Nils Egger
                        <sup>1</sup>
                    </li>
                    <wbr>
                </ul>
            </div>

<div class="col-md-3">
</div>
    <div class="col-md-12 text-center">
        <sup>1</sup>ETH Zurich
        <sup>2</sup>University of Zurich
    </div>
    <br>

    <div class="row text-center">

                <span class="link-block">
            <!-- <a href="https://arxiv.org/abs/2312.02981"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>Code</span>
            </a> -->
            <a href="https://github.com/"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-file-code"></i>
                </span>
                <span>Code</span>
            </a>
            </span>
    </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    In this work, we propose VolDART, an advanced model that integrates VolumetricSMPL into the collision loss function. VolumetricSMPL extends the traditional SMPL model with a volumetric representation of the human body, enabling more accurate and comprehensive collision detection with the scene geometry. This results in more physically plausible motion generation. Compared to the original DART, our method significantly reduces collisions across diverse test scenes. Furthermore, we extend DART to support interactions with dynamic objects and human-human scenarios, demonstrating its versatility in complex, interactive environments.
                </p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <!-- <h4>
                    DART Motion Generation + VolumetricSMPL-based Collision Optimization
                </h4> -->

                <image src="img/method_1.png" width=60% style="display: block; margin: auto;"></image>
                <br>
                <p class="text-justify">
                    We optimized the collision loss in the original DART, introducing VolumetricSMPL to check the collision with SMPL body vertices, while the original DART only considers the joints while optimizing the collision loss.
                </p >
                <br>
            </div>
        </div><br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2" id="area1">
                <h3>
				  Results
                </h3>
                <h4>
                    Scene Navigation
                </h4>

                <div class="text-center ">
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill active" data-value="cabbenchescomp"
                            onclick="selectCompVideo(this, 'area1')"><a>CAB benches</a></li>
                        <li class="method-pill" data-value="cabseminarcomp"
                            onclick="selectCompVideo(this, 'area1')"><a>Seminar-G110</a></li>
                </div>

                <div class="text-center">
                    <div class="video-container">
                        <video class="video" style="height: 280px; max-width: 100%;" id="compVideoArea1" loop playsinline autoplay muted>
                            <source id="sourceArea1" src="videos/h264_cabbenchescomp.mp4" type="video/mp4" />
                        </video>
                    </div>
                    <p class="text-justify" style="text-align: center;">
                        Original DART (left) vs Ours (right) on two EgoBody scenes. Collisions are highlighted in RED. Try selecting scenes!
                    </p>
                    <br>
                </div>
            </div>
            <div class="col-md-8 col-md-offset-2" id="area2">
                <h4>
                    Interaction with Static Objects
                </h4>
                <div class="text-center ">
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill active" data-value="kitchencomp"
                            onclick="selectCompVideo(this, 'area2')"><a>Kitchen</a></li>
                        <li class="method-pill" data-value="foodlab_comp"
                            onclick="selectCompVideo(this, 'area2')"><a>Foodlab</a></li>
                    </ul>
                </div>

                    <!-- <script>
                        activeMethodPill = document.querySelector('.method-pill.active-pill');
                        activeScenePill = document.querySelector('.scene-pill.active-pill');
                        activeModePill = document.querySelector('.mode-pill.active-pill');
                    </script> -->

                <div class="text-center">
                    <div class="video-container">
                        <video class="video" style="height: 280px; max-width: 100%;" id="compVideoArea2" loop playsinline autoplay muted>
                            <source id="sourceArea2" src="videos/h264_kitchencomp.mp4" type="video/mp4" />
                        </video>
                    </div>
                    <p class="text-justify" style="text-align: center; margin-bottom: 2em;">
                        Original DART (left) vs Ours (right) on two EgoBody scenes. Collisions are highlighted in RED color. Try selecting scenes!
                    </p>

                </div>
            <!-- </div> -->
            <br>


            <!-- add other visualizations -->
            <div class="col-md-8 col-md-offset-2" id="area3">
                <h4>
                Human-human Interaction & Interaction with Dynamic Scene
                </h4>
                <p class="text-justify">
                    We also implemented human-human interaction and interaction with dynamic objects. For human-human interaction, we model a high-five motion for two persons by jointly optimizing both humans' motions.
                    For interacting with dynamic objects, we model a person dribbling a bouncing ball.
                </p>
                <div class="text-center ">
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill active" data-value="high_five"
                            onclick="selectCompVideo(this, 'area3')"><a>High-five</a></li>
                        <li class="method-pill" data-value="bouncingball"
                            onclick="selectCompVideo(this, 'area3')"><a>Bouncing Ball</a></li>
                    </ul>
            </div>
            <div class="text-center">
                    <div class="video-container">
                        <video class="video" style="height: 280px; max-width: 100%;" id="compVideoArea3" loop playsinline autoplay muted>
                            <source id="sourceArea3" src="videos/h264_high_five.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div>
            <br>
            </div>


            <div class="col-md-8 col-md-offset-2" id="area3">
                <h3>
				  Quantitative Results
                </h3>
                <h4>
                    Collision Evaluation
                </h4>
                <p class="text-justify">
                Our collision evaluation includes the collision percentage, the total number
                of vertices colliding with the scene among all the evaluated
                frames, the collision reduction precentage.
                </p>
<!--                 <image src="img/pose_est_table.png" width=95% style="display: block; margin: auto;"></image> -->
                <image src="img/collision_new.png" width=100% style="display: block; margin: auto;"></image>
                <p class="text-justify" style="text-align: center;">
                    Original DART (baseline) vs Ours on the navigation task.
                </p>
                <br>

                <h4>
                    Motion Quality Evaluation
                </h4>
                <p class="text-justify">
                Our motion quality evaluation includes the distance to the final goal (in meters), the collision percentage, the total number
                of vertices colliding with the scene among all the evaluated
                frames, the joint loss, the contact motion smoothness, which
                is represented by the jerk loss.
                </p>
                <image src="img/motion_eval.png" width=95% style="display: block; margin: auto;"></image>
                <p class="text-justify" style="text-align: center;">
                    Original DART (baseline) vs Ours on the navigation task.
                </p>
            </div>
            <br>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We would like to thank our supervisor Kaifeng Zhao for his valuable guidance in this Digital Humans project.
                    <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
