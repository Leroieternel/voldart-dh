
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Marigold-3DV</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://marigold-3dv.github.io/img/overview_combined.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1711">
    <meta property="og:image:height" content="576">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://marigold-3dv.github.io/"/>
    <meta property="og:title" content="Marigold: Is it all sunshine and Marigolds?" />
    <meta property="og:description" content="3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches."/>

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ReconFusion: 3D Reconstruction with Diffusion Priors" />
    <meta name="twitter:description" content="3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches."/>
    <meta name="twitter:image" content="https://reconfusion.github.io/img/overview_combined.png" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤”</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
	<link rel="stylesheet" href="css/fontawesome.all.min.css">
	<link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">


	<!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8ZERS5BVPS"></script>
  <script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-8ZERS5BVPS');
  </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
	<script defer src="js/fontawesome.all.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.5.0/Chart.min.js"></script>

    <script src="js/app.js"></script>
    <script src="js/synced_video_selector.js"></script>

</head>

<body style="padding: 1%; width: 100%">
    <div class="container-lg text-center" style="max-width: 1500px; margin: auto;" id="main">
    <!-- <div class="container" id="main"> -->
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Synthesizing Interactive Human Behaviors</b></br>
            </h2>
        </div>
        <div class="row text-center">
<div class="col-md-3">
    </div>
            <div class="col-md-6 text-center">
                <ul class="list-inline">
                    <li>
                            Jixian Tang
                        <sup>1, 2</sup>
                    </li>
                    <li>
                            Xiangyi Jia
                       <sup>1</sup>
                    </li>
                    <li>
                            Nils Egger
                        <sup>1</sup>
                    </li>
                    <wbr>
                </ul>
            </div>

<div class="col-md-3">
</div>
    <div class="col-md-12 text-center">
        <sup>1</sup>ETH Zurich
        <sup>2</sup>University of Zurich
    </div>
    <br>

    <div class="row text-center">

                <span class="link-block">
            <!-- <a href="https://arxiv.org/abs/2312.02981"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>Code</span>
            </a> -->
            <a href="https://github.com/"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-file-code"></i>
                </span>
                <span>Code</span>
            </a>
            </span>
    </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    DART is a diffusion-based autoregressive motion model for real-time text-driven motion control. It is capable of creating realistic sequences of motions conditioned on text.
                    Naturally the model should be capable of adapting to different scene geometry, but also avoid self-intersection for humans. This is where the current optimization approach of latent codes lacks in qualitative results. With our contributions, 
                    the optimization is improved by using SMPL-X and Volumetric SMPL, which gives better collision loss objectives, 
                    leading to less intersections with the scene geometry and self-intersections.
                </p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <!-- <h4>
                    DART Motion Generation + VolumetricSMPL-based Collision Optimization
                </h4> -->

                <image src="img/method_overview.png" width=60% style="display: block; margin: auto;"></image>
                <br>
                <p class="text-justify">
                    To evaluate Marigold performance, we run Marigold on various dataset with groundtruth depth, and align the affine-invariant depth from Marigold with the groundtruth by RANSAC. Then we compare the aligned depth and groundtruth depth qualitatively and quantitatively using absolute relative error.
                </p >
                <br>
            </div>
        </div><br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2" id="area1">
                <h3>
				  Results
                </h3>
                <h4>
                    Scene Navigation
                </h4>

                <div class="text-center ">
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill active" data-value="fortress"
                            onclick="selectCompVideo(this, 'area1')"><a>CAB benches</a></li>
                        <li class="method-pill" data-value="leaves"
                            onclick="selectCompVideo(this, 'area1')"><a>Seminar-G110</a></li>
                </div>

                <div class="text-center">
                    <div class="video-container">
                        <video class="video" style="height: 280px; max-width: 100%;" id="compVideoArea1" loop playsinline autoplay muted>
                            <source id="sourceArea1" src="/Users/jiaxiangyi/Downloads/cab_benches_dart.mp4" type="video/mp4" />
                        </video>
                    </div>
                    <p class="text-justify" style="text-align: center;">
                        Original DART (left) vs Ours (right) on two EgoBody scenes. Collisions are highlighted in RED. Try selecting scenes!
                    </p>
                    <br>
                </div>
            <!-- </div> -->

                <h4>
                    Interaction with Static Objects
                </h4>

            <!-- <div class="row"> -->
            <!-- <div class="col-md-8 col-md-offset-2" id="area2"> -->

                <div class="text-center ">
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill active" data-value="record3d_sheep"
                            onclick="selectCompVideo(this, 'area2')"><a>Kitchen</a></li>
                        <li class="method-pill" data-value="record3d_flower"
                            onclick="selectCompVideo(this, 'area2')"><a>Foodlab</a></li>
                    </ul>
                </div>

                    <!-- <script>
                        activeMethodPill = document.querySelector('.method-pill.active-pill');
                        activeScenePill = document.querySelector('.scene-pill.active-pill');
                        activeModePill = document.querySelector('.mode-pill.active-pill');
                    </script> -->

                <div class="text-center">
                    <div class="video-container">
                        <video class="video" style="height: 280px; max-width: 100%;" id="compVideoArea2" loop playsinline autoplay muted>
                            <source id="sourceArea2" src="videos/h264_record3d_sheep.mp4" type="video/mp4" />
                        </video>
                    </div>
                    <p class="text-justify" style="text-align: center; margin-bottom: 2em;">
                        Original DART (left) vs Ours (right) on two EgoBody scenes. Collisions are highlighted in RED. Try selecting scenes!
                    </p>
                        <!-- <script>
                            video0 = document.getElementById("compVideo0");
                            video0.addEventListener('loadedmetadata', function() {
                                if (activeVidID == 0 && select){
                                    video0.play();
                                    // print video size
                                    console.log(video0.videoWidth, video0.videoHeight);
                                    video0.hidden = false;
                                    video1.hidden = true;
                                }
                            });
                        </script> -->

                        <!-- <script>
                            activeMethodPill = document.querySelector('.method-pill.active-pill');
                        </script> -->
                </div>
            <!-- </div> -->
            <br>

            <h4>
                Human-human Interaction & Interaction with Dynamic Scene
            </h4>

            <!-- add other visualizations -->
            <!-- <div class="col-md-8 col-md-offset-2" id="area3"> -->
                <p class="text-justify">
                    We also implemented human-human interaction and interaction with dynamic objects. For human-human interaction, we model a high-five motion for two persons by jointly optimizing both humans' motions.
                    For interacting with dynamic objects, we model a person dribbling a bouncing ball.
                </p>
                <div class="text-center ">
                    <ul class="nav nav-pills center-pills">
                        <li class="method-pill active" data-value="record3d_sheep"
                            onclick="selectCompVideo(this, 'area3')"><a>High-five</a></li>
                        <li class="method-pill" data-value="record3d_flower"
                            onclick="selectCompVideo(this, 'area3')"><a>Bouncing Ball</a></li>
                    </ul>
            </div>
            <div class="text-center">
                    <div class="video-container">
                        <video class="video" style="height: 280px; max-width: 100%;" id="compVideoArea3" loop playsinline autoplay muted>
                            <source id="sourceArea3" src="videos/h264_record3d_sheep.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div>
            <br>
            </div>


            <div class="col-md-8 col-md-offset-2" id="area3">
                <h3>
				  Quantitative Results
                </h3>
                <h4>
                    Scene Navigation
                </h4>
                <p class="text-justify">
                Our quantative evaluation includes the distance to the final goal (in meters), the collision percentage, the total number
                of vertices colliding with the scene among all the evaluated
                frames, the joint loss, the contact motion smoothness, which
                is represented by the jerk loss.
                </p>
<!--                 <image src="img/pose_est_table.png" width=95% style="display: block; margin: auto;"></image> -->
                <image src="img/frame10_result.png" width=100% style="display: block; margin: auto;"></image>
                <p class="text-justify" style="text-align: center;">
                    Original DART (baseline) vs Ours on the navigation task.
                </p>
                <br>

                <h4>
                    Interaction with Static Objects
                </h4>
                <image src="img/fail_cases.png" width=95% style="display: block; margin: auto;"></image>
                <p class="text-justify" style="text-align: center;">
                    Original DART (baseline) vs Ours on the navigation task.
                </p>
            </div>
            <br>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We would like to thank our supervisor Kaifeng Zhao for his valuable guidance in this Digital Humans project.
                    <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
